"""
–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
"""
import threading
from typing import Dict, List, Set, Tuple, Optional
import numpy as np
from collections import defaultdict
import re

from utils.logger import get_logger
from storage.models import KeywordData

logger = get_logger('WordStat.AI.Clustering')

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import KMeans, AgglomerativeClustering
    SKLEARN_AVAILABLE = True
except ImportError:
    logger.warning("‚ö† scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
    SKLEARN_AVAILABLE = False

try:
    import pymorphy3
    MORPH_AVAILABLE = True
except ImportError:
    logger.warning("‚ö† pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
    MORPH_AVAILABLE = False

# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
RUSSIAN_STOP_WORDS = {
    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ',
    '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ',
    '—Ç–æ–ª—å–∫–æ', '–µ—ë', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â—ë', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É',
    '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '—É–∂–µ', '–¥–ª—è', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–±—ã–ª', '—á–µ–≥–æ', '—Ç—É—Ç', '–∑–¥–µ—Å—å',
    '–µ—Å–ª–∏', '–∏—Ö', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–ø—Ä–∏', '—á–µ–º', '–±–µ–∑', '–¥–æ', '–ø–æ–¥', '–Ω–∞–¥', '–æ–±'
}


class SemanticAnalyzer:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"""
    
    def __init__(self, lemmatize: bool = True, max_features: int = 1000):
        """
        Args:
            lemmatize: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é
            max_features: –ú–∞–∫—Å–∏–º—É–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ TF-IDF
        """
        try:
            self.lemmatize = lemmatize and MORPH_AVAILABLE
            self.max_features = max_features
            self._last_clusters = {}  # –•—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
            if self.lemmatize:
                self.morph = pymorphy3.MorphAnalyzer()
            else:
                self.morph = None
            
            self.vectorizer = None
            self.lock = threading.RLock()
            
            logger.info(f"‚úì SemanticAnalyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (lemmatize: {self.lemmatize}, sklearn: {SKLEARN_AVAILABLE})")
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ SemanticAnalyzer: {e}")
            self.lemmatize = False
            self.morph = None
            self._last_clusters = {}
    
    def _lemmatize_text(self, text: str, remove_stop_words: bool = True) -> str:
        """–õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –∏ —É–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞"""
        try:
            words = text.lower().split()
            result_words = []
            
            for word in words:
                # –£–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
                if remove_stop_words and word in RUSSIAN_STOP_WORDS:
                    continue
                
                try:
                    if self.lemmatize and self.morph:
                        parsed = self.morph.parse(word)[0]
                        lemma = parsed.normal_form
                        result_words.append(lemma)
                    else:
                        result_words.append(word)
                except Exception:
                    result_words.append(word)
            
            return ' '.join(result_words)
        
        except Exception as e:
            logger.debug(f"‚ö† –û—à–∏–±–∫–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏: {e}")
            return text
    
    def analyze(self, keywords: Dict[str, KeywordData], 
                threshold: float = 0.5, 
                n_clusters: int = 10,
                clustering_mode: str = 'threshold',
                min_cluster_size: int = 2) -> Dict[str, List[str]]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        
        Args:
            keywords: –°–ª–æ–≤–∞—Ä—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ (–¥–ª—è threshold —Ä–µ–∂–∏–º–∞)
            n_clusters: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–¥–ª—è fixed —Ä–µ–∂–∏–º–∞)
            clustering_mode: 'threshold' –∏–ª–∏ 'fixed'
            min_cluster_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ {—Ü–µ–Ω—Ç—Ä: [–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞]}
        """
        try:
            if not keywords:
                logger.warning("‚ö† –ü—É—Å—Ç–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return {}
            
            if not SKLEARN_AVAILABLE:
                logger.warning("‚ö† scikit-learn –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞—é –∏—Å—Ö–æ–¥–Ω—ã–µ –∫–ª—é—á–∏")
                return {'–æ—Å–Ω–æ–≤–Ω–æ–π': list(keywords.keys())}
            
            logger.info(f"ü§ñ –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑: {len(keywords)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
            logger.info(f"   Mode: {clustering_mode}, threshold: {threshold}, clusters: {n_clusters}")
            
            with self.lock:
                # ‚úÖ –ü–û–î–ì–û–¢–û–í–ò–¢–¨ –¢–ï–ö–°–¢
                phrases = list(keywords.keys())
                
                logger.info(f"üìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {len(phrases)} —Ñ—Ä–∞–∑...")
                
                processed_phrases = [self._lemmatize_text(p) for p in phrases]
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
                valid_indices = [i for i, p in enumerate(processed_phrases) if p.strip()]
                if len(valid_indices) < len(phrases):
                    logger.info(f"‚ö† –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(phrases) - len(valid_indices)} –ø—É—Å—Ç—ã—Ö —Ñ—Ä–∞–∑")
                    phrases = [phrases[i] for i in valid_indices]
                    processed_phrases = [processed_phrases[i] for i in valid_indices]
                
                if len(phrases) < 2:
                    logger.warning("‚ö† –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ñ—Ä–∞–∑ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
                    return {'–î—Ä—É–≥–æ–µ': phrases} if phrases else {}
                
                logger.info(f"‚úì –§—Ä–∞–∑—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
                
                # ‚úÖ –°–û–ó–î–ê–¢–¨ TF-IDF –ú–ê–¢–†–ò–¶–£ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                logger.info(f"üî¢ –°–æ–∑–¥–∞—é TF-IDF –º–∞—Ç—Ä–∏—Ü—É...")
                
                vectorizer = TfidfVectorizer(
                    max_features=self.max_features,
                    ngram_range=(1, 3),  # –£–ª—É—á—à–µ–Ω–æ: –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ç—Ä–∏–≥—Ä–∞–º–º—ã –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
                    lowercase=True,
                    strip_accents='unicode',
                    min_df=1,
                    max_df=0.95,  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
                    sublinear_tf=True  # –õ—É—á—à–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                )
                
                tfidf_matrix = vectorizer.fit_transform(processed_phrases)
                logger.info(f"‚úì TF-IDF –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞: {tfidf_matrix.shape}")
                
                # ‚úÖ –í–´–ß–ò–°–õ–ò–¢–¨ –°–•–û–î–°–¢–í–û
                logger.info(f"üìä –í—ã—á–∏—Å–ª—è—é —Å—Ö–æ–¥—Å—Ç–≤–æ...")
                similarity_matrix = cosine_similarity(tfidf_matrix)
                logger.info(f"‚úì –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∞")
                
                # ‚úÖ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø
                if clustering_mode == 'fixed':
                    clusters = self._cluster_agglomerative(phrases, tfidf_matrix, n_clusters)
                else:
                    clusters = self._cluster_threshold(phrases, similarity_matrix, threshold)
                
                # ‚úÖ –ü–†–ò–ú–ï–ù–ò–¢–¨ –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô –†–ê–ó–ú–ï–† –ö–õ–ê–°–¢–ï–†–ê
                clusters = self._enforce_min_cluster_size(clusters, min_cluster_size)
                
                logger.info(f"‚úì –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
                
                # –°–æ—Ö—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
                self._last_clusters = clusters
                
                return clusters
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def _cluster_threshold(self, phrases: List[str], 
                          similarity_matrix: np.ndarray,
                          threshold: float = 0.5) -> Dict[str, List[str]]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        try:
            logger.info(f"üîó –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É: {threshold}")
            
            clusters = {}
            assigned = set()
            
            for i, phrase in enumerate(phrases):
                if i in assigned:
                    continue
                
                # ‚úÖ –ù–ê–ô–¢–ò –í–°–ï –ü–û–•–û–ñ–ò–ï –§–†–ê–ó–´
                similar = [i]
                for j in range(i + 1, len(phrases)):
                    if j not in assigned and similarity_matrix[i, j] >= threshold:
                        similar.append(j)
                        assigned.add(j)
                
                assigned.add(i)
                
                # ‚úÖ –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨ –°–ê–ú–£–Æ –ü–û–ü–£–õ–Ø–†–ù–£–Æ –ö–ê–ö –¶–ï–ù–¢–†
                center_phrase = phrase
                cluster_phrases = [phrases[idx] for idx in similar]
                clusters[center_phrase] = cluster_phrases
                
                logger.debug(f"   –ö–ª–∞—Å—Ç–µ—Ä '{center_phrase}': {len(cluster_phrases)} —Ñ—Ä–∞–∑")
            
            return clusters
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ _cluster_threshold: {e}")
            return {}
    
    def _cluster_agglomerative(self, phrases: List[str],
                               tfidf_matrix,
                               n_clusters: int) -> Dict[str, List[str]]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è Agglomerative - –ª—É—á—à–µ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø"""
        try:
            logger.info(f"üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è Agglomerative: {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            
            # ‚úÖ –û–ì–†–ê–ù–ò–ß–ò–¢–¨ –ö–û–õ–ò–ß–ï–°–¢–í–û –ö–õ–ê–°–¢–ï–†–û–í
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ n_clusters –Ω–µ –±–æ–ª—å—à–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ—Ä–∞–∑
            actual_n_clusters = min(n_clusters, len(phrases))
            
            # –ú–∏–Ω–∏–º—É–º 1 –∫–ª–∞—Å—Ç–µ—Ä
            if actual_n_clusters < 1:
                actual_n_clusters = 1
            
            logger.info(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º {actual_n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–∑–∞–ø—Ä–æ—à–µ–Ω–æ: {n_clusters}, —Ñ—Ä–∞–∑: {len(phrases)})")
            
            # ‚úÖ –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨ AGGLOMERATIVE CLUSTERING
            # –û–Ω –ª—É—á—à–µ –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
            clustering = AgglomerativeClustering(
                n_clusters=actual_n_clusters,
                metric='euclidean',
                linkage='ward'
            )
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º sparse matrix –≤ dense –¥–ª—è AgglomerativeClustering
            dense_matrix = tfidf_matrix.toarray()
            labels = clustering.fit_predict(dense_matrix)
            
            # ‚úÖ –°–û–ë–†–ê–¢–¨ –ö–õ–ê–°–¢–ï–†–´
            clusters = defaultdict(list)
            
            for phrase, label in zip(phrases, labels):
                clusters[label].append(phrase)
            
            # ‚úÖ –ù–ê–ô–¢–ò –¶–ï–ù–¢–†–´ –ö–õ–ê–°–¢–ï–†–û–í (–ø–µ—Ä–≤–∞—è —Ñ—Ä–∞–∑–∞ –∫–∞–∫ –Ω–∞–∑–≤–∞–Ω–∏–µ)
            result_clusters = {}
            
            for cluster_id, cluster_phrases in clusters.items():
                # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–≤—É—é —Ñ—Ä–∞–∑—É –∫–∞–∫ –Ω–∞–∑–≤–∞–Ω–∏–µ
                center_phrase = cluster_phrases[0]
                result_clusters[center_phrase] = cluster_phrases
                
                logger.debug(f"   –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} '{center_phrase}': {len(cluster_phrases)} —Ñ—Ä–∞–∑")
            
            return result_clusters
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ _cluster_agglomerative: {e}")
            import traceback
            traceback.print_exc()
            # Fallback to simple grouping
            return {'–î—Ä—É–≥–æ–µ': phrases}
    
    def _enforce_min_cluster_size(self, clusters: Dict[str, List[str]], 
                                   min_size: int = 2) -> Dict[str, List[str]]:
        """
        –ü—Ä–∏–º–µ–Ω–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞.
        –ö–ª–∞—Å—Ç–µ—Ä—ã —Å –º–µ–Ω–µ–µ —á–µ–º min_size –∫–ª—é—á–µ–π –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –≤ '–î—Ä—É–≥–æ–µ'.
        """
        try:
            if not clusters:
                return {}
            
            logger.info(f"üîß –ü—Ä–∏–º–µ–Ω—è—é –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞: {min_size}")
            
            result = {}
            other_phrases = []
            
            for center, phrases in clusters.items():
                if len(phrases) >= min_size:
                    result[center] = phrases
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ "–î—Ä—É–≥–æ–µ"
                    other_phrases.extend(phrases)
                    logger.debug(f"   –ö–ª–∞—Å—Ç–µ—Ä '{center}' ({len(phrases)} —Ñ—Ä–∞–∑) -> –î—Ä—É–≥–æ–µ")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä "–î—Ä—É–≥–æ–µ" –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ
            if other_phrases:
                result['–î—Ä—É–≥–æ–µ'] = other_phrases
                logger.info(f"   üì¶ –ö–ª–∞—Å—Ç–µ—Ä '–î—Ä—É–≥–æ–µ': {len(other_phrases)} —Ñ—Ä–∞–∑")
            
            logger.info(f"‚úì –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(result)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            return result
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ _enforce_min_cluster_size: {e}")
            return clusters
    
    def get_last_clusters(self) -> Dict[str, List[str]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        return getattr(self, '_last_clusters', {})
    
    def get_cluster_stats(self, clusters: Dict[str, List[str]]) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        try:
            if not clusters:
                return {}
            
            stats = {
                'total_clusters': len(clusters),
                'total_keywords': sum(len(v) for v in clusters.values()),
                'avg_cluster_size': sum(len(v) for v in clusters.values()) / len(clusters) if clusters else 0,
                'max_cluster_size': max(len(v) for v in clusters.values()) if clusters else 0,
                'min_cluster_size': min(len(v) for v in clusters.values()) if clusters else 0,
            }
            
            return stats
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ get_cluster_stats: {e}")
            return {}
    
    def format_clusters(self, clusters: Dict[str, List[str]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –≤—ã–≤–æ–¥–∞"""
        try:
            if not clusters:
                return "–ö–ª–∞—Å—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            
            output = []
            output.append("=" * 80)
            output.append("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò")
            output.append("=" * 80)
            
            for idx, (center, phrases) in enumerate(clusters.items(), 1):
                output.append(f"\nüéØ –ö–ª–∞—Å—Ç–µ—Ä {idx}: '{center}'")
                output.append(f"   üìä –†–∞–∑–º–µ—Ä: {len(phrases)} —Ñ—Ä–∞–∑")
                output.append(f"   üìã –§—Ä–∞–∑—ã:")
                
                for phrase in phrases[:10]:  # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 10
                    output.append(f"      ‚Ä¢ {phrase}")
                
                if len(phrases) > 10:
                    output.append(f"      ... –∏ –µ—â—ë {len(phrases) - 10} —Ñ—Ä–∞–∑")
            
            output.append("\n" + "=" * 80)
            
            return "\n".join(output)
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ format_clusters: {e}")
            return str(clusters)
    
    def format_clusters_for_export(self, clusters: Dict[str, List[str]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫)"""
        try:
            if not clusters:
                return "–ö–ª–∞—Å—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            
            output = []
            output.append("–†–ï–ó–£–õ–¨–¢–ê–¢–´ AI –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò")
            output.append("=" * 80)
            output.append("")
            
            for idx, (center, phrases) in enumerate(clusters.items(), 1):
                output.append(f"–ö–õ–ê–°–¢–ï–† {idx}: {center}")
                output.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(phrases)}")
                output.append("-" * 40)
                
                for phrase in phrases:  # –í—Å–µ —Ñ—Ä–∞–∑—ã
                    output.append(f"  {phrase}")
                
                output.append("")
            
            output.append("=" * 80)
            
            return "\n".join(output)
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ format_clusters_for_export: {e}")
            return str(clusters)
    
    def export_clusters_tsv(self, clusters: Dict[str, List[str]]) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã –≤ TSV —Ñ–æ—Ä–º–∞—Ç"""
        try:
            if not clusters:
                return ""
            
            lines = ["–ö–ª–∞—Å—Ç–µ—Ä\t–§—Ä–∞–∑–∞"]
            
            for center, phrases in clusters.items():
                for phrase in phrases:
                    lines.append(f"{center}\t{phrase}")
            
            return "\n".join(lines)
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ export_clusters_tsv: {e}")
            return ""