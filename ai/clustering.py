"""
–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
–í–ï–†–°–ò–Ø 2.0: –° –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Sentence-Transformers –∏ HDBSCAN
"""
import threading
from typing import Dict, List, Set, Tuple, Optional
import numpy as np
from collections import defaultdict
import re

from utils.logger import get_logger
from storage.models import KeywordData

logger = get_logger('WordStat.AI.Clustering')

# ============================================================================
# –ü–†–û–í–ï–†–ö–ê –î–û–°–¢–£–ü–ù–´–• –ë–ò–ë–õ–ò–û–¢–ï–ö
# ============================================================================

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import KMeans, AgglomerativeClustering
    SKLEARN_AVAILABLE = True
except ImportError:
    logger.warning("‚ö† scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
    SKLEARN_AVAILABLE = False

try:
    import pymorphy3
    MORPH_AVAILABLE = True
except ImportError:
    logger.warning("‚ö† pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
    MORPH_AVAILABLE = False

# ‚úÖ SENTENCE-TRANSFORMERS - –õ–£–ß–®–ò–ô –í–ê–†–ò–ê–ù–¢ –î–õ–Ø –°–ï–ú–ê–ù–¢–ò–ö–ò
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
    logger.info("‚úì sentence-transformers –¥–æ—Å—Ç—É–ø–µ–Ω - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤–∫–ª—é—á—ë–Ω")
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logger.warning("‚ö† sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF")

# ‚úÖ HDBSCAN - –õ–£–ß–®–ò–ô –ê–õ–ì–û–†–ò–¢–ú –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò
try:
    import hdbscan
    HDBSCAN_AVAILABLE = True
    logger.info("‚úì HDBSCAN –¥–æ—Å—Ç—É–ø–µ–Ω - —É–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞")
except ImportError:
    HDBSCAN_AVAILABLE = False
    logger.warning("‚ö† hdbscan –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º Agglomerative")

# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
RUSSIAN_STOP_WORDS = {
    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ',
    '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ',
    '—Ç–æ–ª—å–∫–æ', '–µ—ë', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â—ë', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É',
    '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '—É–∂–µ', '–¥–ª—è', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–±—ã–ª', '—á–µ–≥–æ', '—Ç—É—Ç', '–∑–¥–µ—Å—å',
    '–µ—Å–ª–∏', '–∏—Ö', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–ø—Ä–∏', '—á–µ–º', '–±–µ–∑', '–¥–æ', '–ø–æ–¥', '–Ω–∞–¥', '–æ–±'
}


class SemanticAnalyzer:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
    - Sentence-Transformers (–º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏)
    - HDBSCAN (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)
    - Fallback –Ω–∞ TF-IDF + Agglomerative
    """
    
    # –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    EMBEDDING_MODELS = {
        'multilingual': 'paraphrase-multilingual-MiniLM-L12-v2',  # –ë—ã—Å—Ç—Ä–∞—è, —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        'multilingual_large': 'paraphrase-multilingual-mpnet-base-v2',  # –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        'labse': 'sentence-transformers/LaBSE',  # –û—Ç–ª–∏—á–Ω–∞—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
    }
    
    def __init__(self, 
                 lemmatize: bool = True, 
                 max_features: int = 1000,
                 embedding_model: str = 'multilingual',
                 use_semantic: bool = True):
        """
        Args:
            lemmatize: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é –¥–ª—è TF-IDF
            max_features: –ú–∞–∫—Å–∏–º—É–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ TF-IDF (fallback)
            embedding_model: –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ('multilingual', 'multilingual_large', 'labse')
            use_semantic: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
        """
        try:
            self.lemmatize = lemmatize and MORPH_AVAILABLE
            self.max_features = max_features
            self.use_semantic = use_semantic and SENTENCE_TRANSFORMERS_AVAILABLE
            self._last_clusters = {}
            self._clustering_method = 'unknown'
            
            # ‚úÖ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–†–§–û–õ–û–ì–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê
            if self.lemmatize:
                self.morph = pymorphy3.MorphAnalyzer()
            else:
                self.morph = None
            
            # ‚úÖ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø SENTENCE-TRANSFORMERS
            self.sentence_model = None
            if self.use_semantic:
                model_name = self.EMBEDDING_MODELS.get(embedding_model, 
                                                       self.EMBEDDING_MODELS['multilingual'])
                try:
                    logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}...")
                    self.sentence_model = SentenceTransformer(model_name)
                    logger.info(f"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")
                except Exception as e:
                    logger.warning(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF")
                    self.use_semantic = False
                    self.sentence_model = None
            
            self.vectorizer = None
            self.lock = threading.RLock()
            
            method = "Sentence-Transformers + HDBSCAN" if self.use_semantic else "TF-IDF + Agglomerative"
            logger.info(f"‚úì SemanticAnalyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ({method})")
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ SemanticAnalyzer: {e}")
            self.lemmatize = False
            self.morph = None
            self.use_semantic = False
            self.sentence_model = None
            self._last_clusters = {}
    
    def _lemmatize_text(self, text: str, remove_stop_words: bool = True) -> str:
        """–õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –∏ —É–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞"""
        try:
            words = text.lower().split()
            result_words = []
            
            for word in words:
                if remove_stop_words and word in RUSSIAN_STOP_WORDS:
                    continue
                
                try:
                    if self.lemmatize and self.morph:
                        parsed = self.morph.parse(word)[0]
                        lemma = parsed.normal_form
                        result_words.append(lemma)
                    else:
                        result_words.append(word)
                except Exception:
                    result_words.append(word)
            
            return ' '.join(result_words)
        
        except Exception as e:
            logger.debug(f"‚ö† –û—à–∏–±–∫–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏: {e}")
            return text
    
    def _get_semantic_embeddings(self, phrases: List[str]) -> np.ndarray:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø–æ–º–æ—â—å—é Sentence-Transformers.
        –≠—Ç–æ –õ–£–ß–®–ò–ô –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–º—ã—Å–ª–∞ —Ñ—Ä–∞–∑.
        """
        try:
            if not self.sentence_model:
                raise ValueError("Sentence model –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            logger.info(f"üß† –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {len(phrases)} —Ñ—Ä–∞–∑...")
            
            # Sentence-Transformers –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á–∏
            embeddings = self.sentence_model.encode(
                phrases,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True  # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            )
            
            logger.info(f"‚úì –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã: {embeddings.shape}")
            return embeddings
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            raise
    
    def _get_tfidf_embeddings(self, phrases: List[str]) -> np.ndarray:
        """
        Fallback: –ü–æ–ª—É—á–∏—Ç—å TF-IDF —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ sentence-transformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.
        """
        try:
            # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ TF-IDF
            processed_phrases = [self._lemmatize_text(p) for p in phrases]
            
            vectorizer = TfidfVectorizer(
                max_features=self.max_features,
                ngram_range=(1, 3),
                lowercase=True,
                strip_accents='unicode',
                min_df=1,
                max_df=0.95,
                sublinear_tf=True
            )
            
            tfidf_matrix = vectorizer.fit_transform(processed_phrases)
            logger.info(f"‚úì TF-IDF –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞: {tfidf_matrix.shape}")
            
            return tfidf_matrix.toarray()
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ TF-IDF: {e}")
            raise
    
    def _cluster_hdbscan(self, embeddings: np.ndarray, 
                         min_cluster_size: int = 2,
                         min_samples: int = 1) -> np.ndarray:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è HDBSCAN - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.
        –õ—É—á—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.
        """
        try:
            logger.info(f"üéØ HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (min_cluster_size={min_cluster_size})...")
            
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                min_samples=min_samples,
                metric='euclidean',
                cluster_selection_method='eom',  # Excess of Mass
                prediction_data=True
            )
            
            labels = clusterer.fit_predict(embeddings)
            
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = list(labels).count(-1)
            
            logger.info(f"‚úì HDBSCAN: {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, {n_noise} –Ω–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö")
            
            return labels
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ HDBSCAN: {e}")
            raise
    
    def _cluster_agglomerative(self, embeddings: np.ndarray,
                               n_clusters: int) -> np.ndarray:
        """
        Fallback: Agglomerative Clustering —Å –∑–∞–¥–∞–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.
        """
        try:
            actual_n_clusters = min(n_clusters, len(embeddings))
            if actual_n_clusters < 1:
                actual_n_clusters = 1
            
            logger.info(f"üéØ Agglomerative –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: {actual_n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            
            clustering = AgglomerativeClustering(
                n_clusters=actual_n_clusters,
                metric='euclidean',
                linkage='ward'
            )
            
            labels = clustering.fit_predict(embeddings)
            
            logger.info(f"‚úì Agglomerative: {len(set(labels))} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            
            return labels
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ Agglomerative: {e}")
            raise
    
    def _cluster_threshold(self, phrases: List[str], 
                          similarity_matrix: np.ndarray,
                          threshold: float = 0.5) -> Dict[str, List[str]]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É —Å—Ö–æ–¥—Å—Ç–≤–∞ (–¥–ª—è —Ä–µ–∂–∏–º–∞ threshold)"""
        try:
            logger.info(f"üîó –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É: {threshold}")
            
            clusters = {}
            assigned = set()
            
            for i, phrase in enumerate(phrases):
                if i in assigned:
                    continue
                
                similar = [i]
                for j in range(i + 1, len(phrases)):
                    if j not in assigned and similarity_matrix[i, j] >= threshold:
                        similar.append(j)
                        assigned.add(j)
                
                assigned.add(i)
                
                center_phrase = phrase
                cluster_phrases = [phrases[idx] for idx in similar]
                clusters[center_phrase] = cluster_phrases
            
            return clusters
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ _cluster_threshold: {e}")
            return {}
    
    def analyze(self, keywords: Dict[str, KeywordData], 
                threshold: float = 0.5, 
                n_clusters: int = 10,
                clustering_mode: str = 'auto',
                min_cluster_size: int = 2) -> Dict[str, List[str]]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª—É—á—à–∏—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤.
        
        Args:
            keywords: –°–ª–æ–≤–∞—Ä—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ (–¥–ª—è threshold —Ä–µ–∂–∏–º–∞)
            n_clusters: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–¥–ª—è fixed —Ä–µ–∂–∏–º–∞)
            clustering_mode: 'auto', 'semantic', 'tfidf', 'threshold', 'fixed'
            min_cluster_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ {—Ü–µ–Ω—Ç—Ä: [–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞]}
        """
        try:
            if not keywords:
                logger.warning("‚ö† –ü—É—Å—Ç–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return {}
            
            if not SKLEARN_AVAILABLE:
                logger.warning("‚ö† scikit-learn –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
                return {'–æ—Å–Ω–æ–≤–Ω–æ–π': list(keywords.keys())}
            
            logger.info(f"ü§ñ –ù–∞—á–∏–Ω–∞—é AI –∞–Ω–∞–ª–∏–∑: {len(keywords)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
            logger.info(f"   Mode: {clustering_mode}, threshold: {threshold}, n_clusters: {n_clusters}")
            
            with self.lock:
                phrases = list(keywords.keys())
                
                if len(phrases) < 2:
                    logger.warning("‚ö† –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ñ—Ä–∞–∑ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
                    return {'–î—Ä—É–≥–æ–µ': phrases} if phrases else {}
                
                # ‚úÖ –í–´–ë–û–† –ú–ï–¢–û–î–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í
                if clustering_mode in ['auto', 'semantic'] and self.use_semantic:
                    # –õ–£–ß–®–ò–ô –í–ê–†–ò–ê–ù–¢: Sentence-Transformers
                    logger.info("üß† –ò—Å–ø–æ–ª—å–∑—É–µ–º Sentence-Transformers (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏)")
                    self._clustering_method = 'Sentence-Transformers'
                    embeddings = self._get_semantic_embeddings(phrases)
                else:
                    # Fallback: TF-IDF
                    logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF (–ª–µ–∫—Å–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏)")
                    self._clustering_method = 'TF-IDF'
                    embeddings = self._get_tfidf_embeddings(phrases)
                
                # ‚úÖ –í–´–ë–û–† –ú–ï–¢–û–î–ê –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò
                if clustering_mode == 'threshold':
                    # –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
                    similarity_matrix = cosine_similarity(embeddings)
                    clusters = self._cluster_threshold(phrases, similarity_matrix, threshold)
                
                elif clustering_mode == 'fixed':
                    # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    labels = self._cluster_agglomerative(embeddings, n_clusters)
                    clusters = self._labels_to_clusters(phrases, labels)
                
                elif clustering_mode in ['auto', 'semantic'] and HDBSCAN_AVAILABLE:
                    # –õ–£–ß–®–ò–ô –í–ê–†–ò–ê–ù–¢: HDBSCAN
                    logger.info("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º HDBSCAN (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)")
                    self._clustering_method += ' + HDBSCAN'
                    labels = self._cluster_hdbscan(embeddings, min_cluster_size=min_cluster_size)
                    clusters = self._labels_to_clusters(phrases, labels)
                
                else:
                    # Fallback: Agglomerative
                    labels = self._cluster_agglomerative(embeddings, n_clusters)
                    clusters = self._labels_to_clusters(phrases, labels)
                
                # ‚úÖ –ü–†–ò–ú–ï–ù–ò–¢–¨ –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô –†–ê–ó–ú–ï–† –ö–õ–ê–°–¢–ï–†–ê
                clusters = self._enforce_min_cluster_size(clusters, min_cluster_size)
                
                logger.info(f"‚úì –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ({self._clustering_method})")
                
                self._last_clusters = clusters
                return clusters
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def _labels_to_clusters(self, phrases: List[str], 
                           labels: np.ndarray) -> Dict[str, List[str]]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä—å"""
        try:
            clusters_dict = defaultdict(list)
            
            for phrase, label in zip(phrases, labels):
                if label == -1:
                    # HDBSCAN: –Ω–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ -> "–î—Ä—É–≥–æ–µ"
                    clusters_dict['–î—Ä—É–≥–æ–µ'].append(phrase)
                else:
                    clusters_dict[label].append(phrase)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç {center: [phrases]}
            result = {}
            for label, cluster_phrases in clusters_dict.items():
                if label == '–î—Ä—É–≥–æ–µ':
                    result['–î—Ä—É–≥–æ–µ'] = cluster_phrases
                else:
                    center = cluster_phrases[0]  # –ü–µ—Ä–≤–∞—è —Ñ—Ä–∞–∑–∞ –∫–∞–∫ —Ü–µ–Ω—Ç—Ä
                    result[center] = cluster_phrases
            
            return result
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ _labels_to_clusters: {e}")
            return {}
    
    def _enforce_min_cluster_size(self, clusters: Dict[str, List[str]], 
                                   min_size: int = 2) -> Dict[str, List[str]]:
        """
        –ü—Ä–∏–º–µ–Ω–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞.
        –ö–ª–∞—Å—Ç–µ—Ä—ã —Å –º–µ–Ω–µ–µ —á–µ–º min_size –∫–ª—é—á–µ–π –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –≤ '–î—Ä—É–≥–æ–µ'.
        """
        try:
            if not clusters:
                return {}
            
            logger.info(f"üîß –ü—Ä–∏–º–µ–Ω—è—é –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞: {min_size}")
            
            result = {}
            other_phrases = clusters.get('–î—Ä—É–≥–æ–µ', []).copy()
            
            for center, phrases in clusters.items():
                if center == '–î—Ä—É–≥–æ–µ':
                    continue
                    
                if len(phrases) >= min_size:
                    result[center] = phrases
                else:
                    other_phrases.extend(phrases)
                    logger.debug(f"   –ö–ª–∞—Å—Ç–µ—Ä '{center}' ({len(phrases)} —Ñ—Ä–∞–∑) -> –î—Ä—É–≥–æ–µ")
            
            if other_phrases:
                result['–î—Ä—É–≥–æ–µ'] = other_phrases
                logger.info(f"   üì¶ –ö–ª–∞—Å—Ç–µ—Ä '–î—Ä—É–≥–æ–µ': {len(other_phrases)} —Ñ—Ä–∞–∑")
            
            logger.info(f"‚úì –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(result)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            return result
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ _enforce_min_cluster_size: {e}")
            return clusters
    
    def get_clustering_method(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        return getattr(self, '_clustering_method', 'unknown')
    
    def get_last_clusters(self) -> Dict[str, List[str]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        return getattr(self, '_last_clusters', {})
    
    def get_cluster_stats(self, clusters: Dict[str, List[str]]) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        try:
            if not clusters:
                return {}
            
            stats = {
                'total_clusters': len(clusters),
                'total_keywords': sum(len(v) for v in clusters.values()),
                'avg_cluster_size': sum(len(v) for v in clusters.values()) / len(clusters) if clusters else 0,
                'max_cluster_size': max(len(v) for v in clusters.values()) if clusters else 0,
                'min_cluster_size': min(len(v) for v in clusters.values()) if clusters else 0,
                'clustering_method': self.get_clustering_method(),
            }
            
            return stats
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ get_cluster_stats: {e}")
            return {}
    
    def format_clusters(self, clusters: Dict[str, List[str]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –≤—ã–≤–æ–¥–∞"""
        try:
            if not clusters:
                return "–ö–ª–∞—Å—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            
            output = []
            output.append("=" * 80)
            output.append(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò ({self.get_clustering_method()})")
            output.append("=" * 80)
            
            for idx, (center, phrases) in enumerate(clusters.items(), 1):
                output.append(f"\nüéØ –ö–ª–∞—Å—Ç–µ—Ä {idx}: '{center}'")
                output.append(f"   üìä –†–∞–∑–º–µ—Ä: {len(phrases)} —Ñ—Ä–∞–∑")
                output.append(f"   üìã –§—Ä–∞–∑—ã:")
                
                for phrase in phrases[:10]:
                    output.append(f"      ‚Ä¢ {phrase}")
                
                if len(phrases) > 10:
                    output.append(f"      ... –∏ –µ—â—ë {len(phrases) - 10} —Ñ—Ä–∞–∑")
            
            output.append("\n" + "=" * 80)
            
            return "\n".join(output)
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ format_clusters: {e}")
            return str(clusters)
    
    def format_clusters_for_export(self, clusters: Dict[str, List[str]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫)"""
        try:
            if not clusters:
                return "–ö–ª–∞—Å—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            
            output = []
            output.append(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´ AI –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò ({self.get_clustering_method()})")
            output.append("=" * 80)
            output.append("")
            
            for idx, (center, phrases) in enumerate(clusters.items(), 1):
                output.append(f"–ö–õ–ê–°–¢–ï–† {idx}: {center}")
                output.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(phrases)}")
                output.append("-" * 40)
                
                for phrase in phrases:
                    output.append(f"  {phrase}")
                
                output.append("")
            
            output.append("=" * 80)
            
            return "\n".join(output)
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ format_clusters_for_export: {e}")
            return str(clusters)
    
    def export_clusters_tsv(self, clusters: Dict[str, List[str]]) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã –≤ TSV —Ñ–æ—Ä–º–∞—Ç"""
        try:
            if not clusters:
                return ""
            
            lines = ["–ö–ª–∞—Å—Ç–µ—Ä\t–§—Ä–∞–∑–∞"]
            
            for center, phrases in clusters.items():
                for phrase in phrases:
                    lines.append(f"{center}\t{phrase}")
            
            return "\n".join(lines)
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ export_clusters_tsv: {e}")
            return ""
    
    @staticmethod
    def is_semantic_available() -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        return SENTENCE_TRANSFORMERS_AVAILABLE
    
    @staticmethod
    def is_hdbscan_available() -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å HDBSCAN"""
        return HDBSCAN_AVAILABLE
    
    @staticmethod
    def get_available_methods() -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        methods = ['tfidf', 'threshold', 'fixed']
        
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            methods.insert(0, 'semantic')
            methods.insert(0, 'auto')
        
        return methods