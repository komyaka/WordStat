"""
–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
"""
import threading
from typing import Dict, List, Set, Tuple, Optional
import numpy as np
from collections import defaultdict
import re

from utils.logger import get_logger
from storage.models import KeywordData

logger = get_logger('WordStat.AI.Clustering')

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import KMeans, DBSCAN
    SKLEARN_AVAILABLE = True
except ImportError:
    logger.warning("‚ö† scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∫ÔøΩÔøΩ–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
    SKLEARN_AVAILABLE = False

try:
    import pymorphy3
    MORPH_AVAILABLE = True
except ImportError:
    logger.warning("‚ö† pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
    MORPH_AVAILABLE = False


class SemanticAnalyzer:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"""
    
    def __init__(self, lemmatize: bool = True, max_features: int = 1000):
        """
        Args:
            lemmatize: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é
            max_features: –ú–∞–∫—Å–∏–º—É–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ TF-IDF
        """
        try:
            self.lemmatize = lemmatize and MORPH_AVAILABLE
            self.max_features = max_features
            
            if self.lemmatize:
                self.morph = pymorphy3.MorphAnalyzer()
            else:
                self.morph = None
            
            self.vectorizer = None
            self.lock = threading.RLock()
            
            logger.info(f"‚úì SemanticAnalyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (lemmatize: {self.lemmatize}, sklearn: {SKLEARN_AVAILABLE})")
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ SemanticAnalyzer: {e}")
            self.lemmatize = False
            self.morph = None
    
    def _lemmatize_text(self, text: str) -> str:
        """–õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç"""
        try:
            if not self.lemmatize or not self.morph:
                return text
            
            words = text.split()
            lemmas = []
            
            for word in words:
                try:
                    parsed = self.morph.parse(word)[0]
                    lemma = parsed.normal_form
                    lemmas.append(lemma)
                except Exception:
                    lemmas.append(word)
            
            return ' '.join(lemmas)
        
        except Exception as e:
            logger.debug(f"‚ö† –û—à–∏–±–∫–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏: {e}")
            return text
    
    def analyze(self, keywords: Dict[str, KeywordData], 
                threshold: float = 0.5, 
                n_clusters: int = 10,
                clustering_mode: str = 'threshold') -> Dict[str, List[str]]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        
        Args:
            keywords: –°–ª–æ–≤–∞—Ä—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ (–¥–ª—è threshold —Ä–µ–∂–∏–º–∞)
            n_clusters: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–¥–ª—è fixed —Ä–µ–∂–∏–º–∞)
            clustering_mode: 'threshold' –∏–ª–∏ 'fixed'
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ {—Ü–µ–Ω—Ç—Ä: [–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞]}
        """
        try:
            if not keywords:
                logger.warning("‚ö† –ü—É—Å—Ç–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return {}
            
            if not SKLEARN_AVAILABLE:
                logger.warning("‚ö† scikit-learn –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞—é –∏—Å—Ö–æ–¥–Ω—ã–µ –∫–ª—é—á–∏")
                return {'–æ—Å–Ω–æ–≤–Ω–æ–π': list(keywords.keys())}
            
            logger.info(f"ü§ñ –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑: {len(keywords)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
            logger.info(f"   Mode: {clustering_mode}, threshold: {threshold}, clusters: {n_clusters}")
            
            with self.lock:
                # ‚úÖ –ü–û–î–ì–û–¢–û–í–ò–¢–¨ –¢–ï–ö–°–¢
                phrases = list(keywords.keys())
                
                logger.info(f"üìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {len(phrases)} —Ñ—Ä–∞–∑...")
                
                if self.lemmatize:
                    processed_phrases = [self._lemmatize_text(p) for p in phrases]
                else:
                    processed_phrases = phrases
                
                logger.info(f"‚úì –§—Ä–∞–∑—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
                
                # ‚úÖ –°–û–ó–î–ê–¢–¨ TF-IDF –ú–ê–¢–†–ò–¶–£
                logger.info(f"üî¢ –°–æ–∑–¥–∞—é TF-IDF –º–∞—Ç—Ä–∏—Ü—É...")
                
                vectorizer = TfidfVectorizer(
                    max_features=self.max_features,
                    ngram_range=(1, 2),
                    lowercase=True,
                    strip_accents='unicode'
                )
                
                tfidf_matrix = vectorizer.fit_transform(processed_phrases)
                logger.info(f"‚úì TF-IDF –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞: {tfidf_matrix.shape}")
                
                # ‚úÖ –í–´–ß–ò–°–õ–ò–¢–¨ –°–•–û–î–°–¢–í–û
                logger.info(f"üìä –í—ã—á–∏—Å–ª—è—é —Å—Ö–æ–¥—Å—Ç–≤–æ...")
                similarity_matrix = cosine_similarity(tfidf_matrix)
                logger.info(f"‚úì –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∞")
                
                # ‚úÖ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø
                if clustering_mode == 'threshold':
                    clusters = self._cluster_threshold(phrases, similarity_matrix, threshold)
                else:
                    clusters = self._cluster_kmeans(phrases, tfidf_matrix, n_clusters)
                
                logger.info(f"‚úì –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
                
                return clusters
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def _cluster_threshold(self, phrases: List[str], 
                          similarity_matrix: np.ndarray,
                          threshold: float = 0.5) -> Dict[str, List[str]]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        try:
            logger.info(f"üîó –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É: {threshold}")
            
            clusters = {}
            assigned = set()
            
            for i, phrase in enumerate(phrases):
                if i in assigned:
                    continue
                
                # ‚úÖ –ù–ê–ô–¢–ò –í–°–ï –ü–û–•–û–ñ–ò–ï –§–†–ê–ó–´
                similar = [i]
                for j in range(i + 1, len(phrases)):
                    if j not in assigned and similarity_matrix[i, j] >= threshold:
                        similar.append(j)
                        assigned.add(j)
                
                assigned.add(i)
                
                # ‚úÖ –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨ –°–ê–ú–£–Æ –ü–û–ü–£–õ–Ø–†–ù–£–Æ –ö–ê–ö –¶–ï–ù–¢–†
                center_phrase = phrase
                cluster_phrases = [phrases[idx] for idx in similar]
                clusters[center_phrase] = cluster_phrases
                
                logger.debug(f"   –ö–ª–∞—Å—Ç–µ—Ä '{center_phrase}': {len(cluster_phrases)} —Ñ—Ä–∞–∑")
            
            return clusters
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ _cluster_threshold: {e}")
            return {}
    
    def _cluster_kmeans(self, phrases: List[str],
                       tfidf_matrix: np.ndarray,
                       n_clusters: int) -> Dict[str, List[str]]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è K-Means"""
        try:
            logger.info(f"üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è K-Means: {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            
            # ‚úÖ –û–ì–†–ê–ù–ò–ß–ò–¢–¨ –ö–û–õ–ò–ß–ï–°–¢–í–û –ö–õ–ê–°–¢–ï–†–û–í
            n_clusters = min(n_clusters, len(phrases))
            
            if n_clusters < 1:
                n_clusters = 1
            
            # ‚úÖ –ó–ê–ü–£–°–¢–ò–¢–¨ K-MEANS
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            labels = kmeans.fit_predict(tfidf_matrix)
            
            # ‚úÖ –°–û–ë–†–ê–¢–¨ –ö–õ–ê–°–¢–ï–†–´
            clusters = defaultdict(list)
            
            for phrase, label in zip(phrases, labels):
                clusters[label].append(phrase)
            
            # ‚úÖ –ù–ê–ô–¢–ò –¶–ï–ù–¢–†–´ –ö–õ–ê–°–¢–ï–†–û–í
            result_clusters = {}
            
            for cluster_id, cluster_phrases in clusters.items():
                # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–≤—É—é —Ñ—Ä–∞–∑—É –∫–∞–∫ –Ω–∞–∑–≤–∞–Ω–∏–µ
                center_phrase = cluster_phrases[0]
                result_clusters[center_phrase] = cluster_phrases
                
                logger.debug(f"   –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} '{center_phrase}': {len(cluster_phrases)} —Ñ—Ä–∞–∑")
            
            return result_clusters
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ _cluster_kmeans: {e}")
            return {}
    
    def get_cluster_stats(self, clusters: Dict[str, List[str]]) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        try:
            if not clusters:
                return {}
            
            stats = {
                'total_clusters': len(clusters),
                'total_keywords': sum(len(v) for v in clusters.values()),
                'avg_cluster_size': sum(len(v) for v in clusters.values()) / len(clusters) if clusters else 0,
                'max_cluster_size': max(len(v) for v in clusters.values()) if clusters else 0,
                'min_cluster_size': min(len(v) for v in clusters.values()) if clusters else 0,
            }
            
            return stats
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ get_cluster_stats: {e}")
            return {}
    
    def format_clusters(self, clusters: Dict[str, List[str]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –≤—ã–≤–æ–¥–∞"""
        try:
            if not clusters:
                return "–ö–ª–∞—Å—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            
            output = []
            output.append("=" * 80)
            output.append("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò")
            output.append("=" * 80)
            
            for idx, (center, phrases) in enumerate(clusters.items(), 1):
                output.append(f"\nüéØ –ö–ª–∞—Å—Ç–µ—Ä {idx}: '{center}'")
                output.append(f"   üìä –†–∞–∑–º–µ—Ä: {len(phrases)} —Ñ—Ä–∞–∑")
                output.append(f"   üìã –§—Ä–∞–∑—ã:")
                
                for phrase in phrases[:10]:  # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 10
                    output.append(f"      ‚Ä¢ {phrase}")
                
                if len(phrases) > 10:
                    output.append(f"      ... –∏ –µ—â—ë {len(phrases) - 10} —Ñ—Ä–∞–∑")
            
            output.append("\n" + "=" * 80)
            
            return "\n".join(output)
        
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ format_clusters: {e}")
            return str(clusters)